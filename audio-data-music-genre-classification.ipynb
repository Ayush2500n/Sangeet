{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; text-align: center;\">\n    <img src = \"https://i.imgur.com/AbTwM3D.png\" width = 450, height= 450>\n    <div style=\"font-size: 62px; color: #121212; text-shadow: 2px 4px 8px rgba(0,0,0,0.2);\n                margin-top: 0;\"><b>Audio Data: Music Genre Classification</b></div>\n        <hr style=\"border: none;\n               border-top: 1.25px solid #121212;\n               width: auto;\n               margin-top: 0;\n               margin-bottom: 0;\n               margin-left: auto;\n               margin-right: auto;\n               height: 0;\">\n    <div style=\"font-weight: bold;\n                text-transform: uppercase;\n                margin-top: 50px;\n                letter-spacing: 2.5px;\n                color: #121212;\n                \">2023 | <a href =\"https://www.kaggle.com/lusfernandotorres/\">¬© Luis Fernando Torres</a></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<br><br><br>\n<div style=\"font-family: 'Segoe UI',sans-serif; text-align: left; font-size: 16px; letter-spacing: 1.5px; color: #121212; margin-top: 25px; margin-bottom: 25px;\">\n    <b>Table of Contents</b>\n</div>\n<br>\n\n- [Introduction](#intro)<br><br>\n    - [Audio Data](#audio-data)<br><br>\n    - [The Model](#architecture)<br><br>\n    - [The Dataset](#dataset)<br><br>\n- [Exploratory Data Analysis](#EDA)<br><br>\n- [Audio Preprocessing](#preprocessing)<br><br>\n- [Fine-tuning HuBERT](#fine-tuning)<br><br>\n- [Conclusion](#conclusion)<br><br>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'intro'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 56px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\n                border-bottom: 1.25px solid #121212\"><b>Introduction</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img style=\"float:left; margin-top: 0;\n    margin:0; \n    padding:20px;\" src = \"https://i.imgur.com/gezmKdI.png\" width = 230, height= 230>\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">In the last few years, we have seen how <b>Deep Learning</b> has been able to bring revolutionary solutions across many industries on tasks related to image and text data. But besides the deeply explored fields of Natural Language Processing and Computer Vision, deep learning also allowed us to explore <b>audio data</b> in several ways.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Audio classification is only one among several applications of Hugging Face's <b>ü§óTransformers</b> in audio processing. It is particularly no that much different from other classification tasks in typical machine learning projects, where one or more labels must be assigned to a particular sample in the dataset.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">In audio processing, classification tasks might be based on identifying what is the language spoken in an audio recording, or detecting key words like <i>‚ùùHey Siri‚ùû</i> to start a conversation with your personal assistant in your phone.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Other than that, we also have several other examples where audio processing with Machine Learning can help us on a day-to-day basis.</p>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"margin-left: 25px;\">\n     \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">‚Ä¢ <b>Healthcare:</b> You can analyze cough sounds to detect possible respiratory conditions, or even detect stress through vocal patterns.</p>\n    \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">‚Ä¢ <b>Automotive:</b> In-car voice command assists in performing user commands for hands-free operation.</p>\n    \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">‚Ä¢ <b>Smart Homes:</b> Recognizing noises like glass breaking or alarms to detect emergencies. It can also be used for interpreting user commands for many home appliances.</p>\n    \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">‚Ä¢ <b>Industrial:</b> The analysis of sounds emitted by machines can help in monitoring possible maintenance necessities before failures happen.</p>\n    \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">‚Ä¢ <b>Communication:</b> Speech-to-Text applications, as well as Text-to-Speech applications. Noise cancellation can also be used to filter out background noise to improve communication quality.</p>\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">In <b>this notebook</b>, we will walk through the process of using a pre-trained audio transformers for <b>audio classification</b> tasks. We will fine-tune a transformer model to perform <b>music genre classification</b>, where our model receives a music recording as input and labels it as genres like <i>pop</i> or <i>rock</i>.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">This kind of task is very present in streaming platforms like <b>Spotify</b> to recommend songs that are similar to the ones the user is currently listening to.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The process you will see in this notebook is based on the <b><a href = \"https://huggingface.co/learn/audio-course/chapter0/introduction\">Hugging Face Audio Course</a></b>, which is available for free. I highly suggest you take the course for a more in-depth view into how to work with audio data for deep learning taks.</p>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'audio-data'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\"><b>Audio Data</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Before delving into the task we have at hand, let's first explore what exactly we refer to when talking about audio data.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">You might already be familiar with the concept of <b>sound waves</b>. These are vibrations that travel through air and are detected by our ears, which in turn help them to travel to our brain to interpret them as sounds.</p>\n         \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The problem with sound waves is that they are <b>continuous</b> signals consisting of an infinite number of values over time, which makes it difficult for digital devices to process and store them. To make it easier to work with them, we use <b>digital representation</b> of sound waves, that converts these infinite signal values into a series of <b>discreet values</b>.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">To get to this digital representation we perform something called <b>sampling</b>, which is the process of measuring the value of a continuous signal at fixed time samples. The <b>sampling rate/frequency</b> is a measure in $hertz$ ($Hz$) that describes the number of samples taken from an audio file in a one-second time frame . A CD-quality audio, for example, has a sampling rate of $44,100$ $Hz$, meaning that 44,100 sample are taken for every second of the recording.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">It is important to say that <b>every audio example</b> in a dataset must have the exact same sampling rate when working on any audio task. If you intent to use custom audio data to fine-tune a pre-trained model, the sampling rate of your data <b>must match</b> the sampling rate of the data the model was pre-trained on. One fundamental step of audio data preprocessing is called <b>Resampling</b>, which is exactly the process of making the sampling rates across the dataset match.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Another important aspect of audio data is the <b>amplitude</b>. In digital audio, each audio sample records the amplitute of the audio wave at a certain point in time. The amplitude can be understood as the perception of <b>loudness</b>. It is measured in $decilbels$ ($dB$) and what it describes is the sound pressure levels at any given instant.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The higher the amplitude, the louder the sound. While a normal speaking voice tends to be under 60 $dB$, a rock concert can be at around 125 $dB$.</p>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'architecture'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\"><b>The Model</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">For this task, we are going to leverage the <b>Transformer architecture</b> to work with audio processing. Let's take a quick recap on what is the transformer architecture.</p>","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src = \"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg\">\n<p style = \"font-size: 16px;\n            font-family: 'Segoe UI', sans-serif;\n            text-align: center;\n            margin-top: 10px;\">Transformer architecture. <br> Source: <a href = \"https://huggingface.co/learn/audio-course/chapter3/introduction\">Hugging Face</a></p>\n</center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The original Transformer architecture consists of an <b>encoder</b> and a <b>decoder</b>. The encoder's role is to extract and process understanding from the input data. In contrast, the decoder generates outputs based on both the data received from the encoder and its own previously generated sequences.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The main element behind the power of transformer models are the <b>attention layers</b>, which aids the model in paying attention to certain elements in the input data while ignoring what may not be relevant enough for the task at hand.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Some transformer-based models tend to use both the encoder and the decoder, whereas others use only one of the two. Encoder-only models might be good for tasks that require a deeper understanding of the input data, such as classification tasks. Decoder-only models tend to be very good at the task of generating texts, such as summarization or translation.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Even though the original Transformer architecture was designed to work with text data, we already have many architectures designed to work with audio data. These models may differ on the type of audio data they take as input and the nuances inside their functioning. Overall, however, they tend to follow a very similar approach to that of the original Transformer.</p>","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src = \"https://i.imgur.com/1UDwGIz.png\">\n<p style = \"font-size: 16px;\n            font-family: 'Segoe UI', sans-serif;\n            text-align: center;\n            margin-top: 10px;\">Transformer architecture for audio input. <br>Source: <a href = \"https://huggingface.co/learn/audio-course/chapter3/introduction\">Hugging Face</a></p>\n</center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">For our specific task, which is focused on labeling the genres of the music played in an audio file, we will need a model that takes audio as input and gives us as output what is the most probable genre/label to that song.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The <b>HuBERT</b> model, proposed in the 2021 research paper <i><a href = \"https://arxiv.org/pdf/2106.07447.pdf\">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></i> by Hsu et al., is an <b>encoder-only</b> model that is ideal for the task of audio classification.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">As the name suggests, HuBERT works very closely with how the BERT model works. The main idea is to <b>mask</b> certain parts of the input audio, and the model is trained to learn how to predict these masked segments. It also uses <b>cluster ensembles</b> to generate pseudo labels that help in guiding the learning process in predicting the masked parts.</p>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'dataset'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\"><b>The Dataset</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">In this notebook, we are going to use the <a href = \"https://huggingface.co/datasets/marsyas/gtzan\">GTZAN</a> dataset to fine-tune the model. </p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Briefly introducing, this dataset contains over 500 samples of 30-seconds long audio files consisting of music from several genres.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's start by loading all the necessary libraries.</p>","metadata":{}},{"cell_type":"code","source":"!pip install evaluate # Installing the evaluate lib","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-24T07:32:48.943208Z","iopub.execute_input":"2024-07-24T07:32:48.943966Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c79d76349a0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/evaluate/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c79d7634cd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/evaluate/\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Importing Libraries\n\n# Data Handling\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset, load_dataset\nimport shutil\nfrom IPython.display import Audio\n\n# Data Visualization\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.subplots as sp\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nfrom IPython.display import display\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\nimport librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Statistics & Mathematics\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro, skew, anderson, kstest, gaussian_kde,spearmanr\nimport math\n\n# PyTorch\nimport torch\n\n# Hiding warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformers\nfrom transformers import AutoFeatureExtractor\nfrom transformers import AutoModelForAudioClassification\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\n\n# Evaluate\nimport evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking if GPU is available\nif torch.cuda.is_available():\n    print(\"GPU is available. \\nUsing GPU\")\n    device = torch.device('cuda')\nelse:\n    print(\"GPU is not available. \\nUsing CPU\")\n    device = torch.device('cpu')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi # Checking GPU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuring notebook\nseed = 42\npaper_color = '#f5f7f6'\nbg_color = '#f5f7f6'\ncolormap = 'cividis'\n# template = ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id = 'EDA'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 56px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\n                border-bottom: 1.25px solid #121212\"><b>Exploratory Data Analysis</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img style=\"float:left; margin-top: 0;\n    margin:0; \n    padding:20px;\" src = \"https://i.imgur.com/2rKStBM.png\" width = 230, height= 230>\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The exploratory analysis of audio data is quite different from what you usually perform on other types of data. If I migh say so, it is extremely interesting and quite fun to do it. Did you know that you could <i>see</i> sound? Well, this is precisely what we are going to do here.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's start by using the ü§ó Datasets library to load our data.</p>","metadata":{}},{"cell_type":"code","source":"# Loading data with the Datasets lib\ndf = load_dataset(\"marsyas/gtzan\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df # Visualizing the dataset","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">You can see that this dataset is not divided into training and validation sets, meaning we will have to split it later on. Overall, we have three diferent features, and <code>num_rows</code> tells us that we have <b>999 audio files</b>.</p>\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's start by creating a validation set, so we can explore the training set.</p>","metadata":{}},{"cell_type":"code","source":"# Splitting data into training and validation\ndf = df['train'].train_test_split(seed = seed, shuffle = True, \n                                  test_size = .2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">With <code>shuffle = True</code> we ensure that samples are split into training and test randomly. With <code>test_size = .2</code> we ensure to use 80% of the data for training and 20% of it for validation.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's see the dataset again.</p>","metadata":{}},{"cell_type":"code","source":"df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Now we have a train dataset with 799 samples and a test dataset with 200 samples. Let's take a look at one of the audio files.</p>","metadata":{}},{"cell_type":"code","source":"# Picking the third file in the training dataset\nprint(\"file:\", df['train'][2]['file']) \nprint('\\n')\nprint(\"audio: \", df['train'][2]['audio']) \nprint('\\n')\nprint(\"genre:\", df['train'][2]['genre']) \nprint('\\n')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">We can se that <code>file</code> describes the audio path, while <code>audio</code> contains some more information.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Besides the path, we can also see that the audio files are represented as 1-dimensional arrays, where the value of the array represents the <b>amplitude</b> at that particular timestep.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">This particular song has a sampling rate of 22,050 $Hz$. We will confirm later on if all files are within that same sampling rate or not.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The music genre is currently represented as an integer. Let's fix that and obtain their labels in a format that is human-readable.</p>","metadata":{}},{"cell_type":"code","source":"# Obtaining human-readable label\nid2label_function = df['train'].features['genre'].int2str","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"genre: \", id2label_function(df['train'][2]['genre']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">We can see that the id <code>3</code> refers to the <code>disco</code> genre.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's investigate if all sampling rates is the same across the dataset.</p>","metadata":{}},{"cell_type":"code","source":"# Initializing variables\nsampling_rate_check = None\nall_same = True\n\n# Iterating through each sample\nfor set_name in ['train', 'test']: # Iterating through both sets\n    for sample in df[set_name]:\n        sampling_rate = sample['audio']['sampling_rate']\n\n        if sampling_rate_check is None:\n            sampling_rate_check = sampling_rate\n        else:\n            if sampling_rate != sampling_rate_check:\n                all_same = False\n            break\n        \n# Printing result\nif all_same:\n    print(f\"All samples have the same sampling rate: {sampling_rate_check} Hz\")\nelse:\n    print(\"The samples in the dataframe have different sampling rates.\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">We have a confirmation that all audio files in both sets share the same $22,050$ $Hz$ sampling rate.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's now count the labels.</p>","metadata":{}},{"cell_type":"code","source":"labels = {} # Initializing empty list\n\n# Defining a function to count labels \ndef count_genres(df):\n    for sample in df:\n        genre_label = id2label_function(sample['genre'])\n        \n        if genre_label in labels:\n            labels[genre_label] += 1\n            \n        else:\n            labels[genre_label] = 1\n            \n# Counting labels in both sets\ncount_genres(df['train'])\ncount_genres(df['test'])\n\n# Obtaining genres and count values\ngenres = list(labels.keys())\ncounts = list(labels.values())\n\n# Plotting bar plot \nfig = px.bar(x = genres, y = counts, text = counts)\nfig.update_traces(marker=dict(color=\"#4682B4\"))\nfig.update_layout(title = '<b>Genre Counts</b>', showlegend = True, height = 500, width = 750,\n                 plot_bgcolor=bg_color,paper_bgcolor=paper_color)\nfig.update_yaxes(title_text = '<b>Count</b>')\nfig.update_xaxes(title = '<b>Genres</b>')\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Overall we have 100 samples for Rock, Disco, Country, Classical Music, Blues, Reggae, HipHop, Pop, and Metal. Jazz is the only genre with only 99 samples, this is due to a corrupted file that was removed from the dataset.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">After obtaining a bit more of insight about the data, we can now start the process of <i>\"seeing\"</i> our data, as I have mentioned before. The more efficient way of doing this is through <b>waveforms</b>, which can be plotted using the <a href = \"https://librosa.org/doc/latest/index.html\">librosa library</a>.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">A waveform illustrates changes in the amplitude of sound over time. In the <i>x-axis</i> we have the time feature, whereas in the <i>y-axis</i> we have amplitude values in a $[-1.00, 1.00]$ range. Let's take a look at some examples, listen to the files, and check their genres to see how they differ from one another.</p>","metadata":{}},{"cell_type":"code","source":"# Selecting array sound & sampling rate\narray = df['train'][2]['audio']['array']\nsampling_rate = df['train'][2]['audio']['sampling_rate']\n\n# Plotting waveform with librosa\nplt.figure().set_figwidth(12)\nplt.title('Waveforms - Example 1')\nlibrosa.display.waveshow(array, sr = sampling_rate)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying audio file\nAudio(data = array, rate = sampling_rate)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing genre\nprint(\"genre: \", id2label_function(df['train'][2]['genre']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array = df['train'][14]['audio']['array']\nsampling_rate = df['train'][14]['audio']['sampling_rate']\n\nplt.figure().set_figwidth(12)\nplt.title('Waveforms - Example 2')\nlibrosa.display.waveshow(array, sr = sampling_rate)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(data = array, rate = sampling_rate)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"genre: \", id2label_function(df['train'][14]['genre']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array = df['train'][42]['audio']['array']\nsampling_rate = df['train'][42]['audio']['sampling_rate']\n\nplt.figure().set_figwidth(12)\n\nplt.title('Waveforms - Example 3')\nlibrosa.display.waveshow(array, sr = sampling_rate)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(data = array, rate = sampling_rate)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"genre: \", id2label_function(df['train'][42]['genre']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"array = df['train'][212]['audio']['array']\nsampling_rate = df['train'][212]['audio']['sampling_rate']\n\nplt.figure().set_figwidth(12)\n\nplt.title('Waveforms - Example 4')\nlibrosa.display.waveshow(array, sr = sampling_rate)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(data = array, rate = sampling_rate)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"genre: \", id2label_function(df['train'][212]['genre']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Right above, we have four audio files from different genres: <code>disco</code>, <code>reggae</code>, <code>rock</code>, and <code>metal</code>. By looking at the waveforms, you can observe that the <code>reggae</code> example seems to be the <i>quieter</i>, with an amplitude ranging from -0.75 to 0.75 and lesser peaks to the extreme values of the range throughout the excerpt.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The <code>disco</code> excerpt seems to be the one with the highest peaks of amplitude, reaching the maximum of -1.00 and 1.00. This could be a sign of <i>clipping</i>‚Äîa distortion that occurs whenever a signal exceeds the maximum level that can be accurately recorded.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">If you listen to the <code>metal</code> excerpt while listening to the audio, you will notice the song gets significantly <i>quieter</i> around the 13-second time frame, which is accurately captured in the waveform when there's a sudden reduction in the range of the peaks.</p>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'preprocessing'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 56px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\n                border-bottom: 1.25px solid #121212\"><b>Audio Preprocessing</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img style=\"float:left; margin-top: 0;\n    margin:0; \n    padding:20px;\" src = \"https://i.imgur.com/bm6Keu0.png\" width = 230, height= 230>\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">After analysing the data, we must prepare it to be used as input for fine-tuning our model. Similar to the tokenization process in Natural Language Processing, audio models require the input to be encoded in a format that the model can understand.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Luckily, the ü§ó Transformers library has a class called <code>AutoFeatureExtractor</code> that is efficient in selecting the correct feature extractor for any given model. </p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">As I have mentioned in the introduction, we are going to use the HuBERT model for this task. However, I will use the DistilHuBERT model, which is a much smaller‚Äîthus <i>distilled</i>‚Äîversion of the original model.</p>","metadata":{}},{"cell_type":"code","source":"# Loading model\nmodel = \"ntu-spml/distilhubert\"\n\n# Instantiating feature extractor\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model, do_normalize = True, return_attention_mask = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">We need to ensure that the sampling rate of the model matches the $22,050$ $Hz$ sampling rate of the dataset. We can use <code>feature_extractor</code> to check the sampling rate of the model.</p>","metadata":{}},{"cell_type":"code","source":"# Checking sampling rate\nsampling_rate = feature_extractor.sampling_rate\nprint(f'DistilHuBERT Sampling Rate: {sampling_rate} Hz')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">We have different sampling rates. This is not a big issue. Let's just use the <code>cast_column()</code> method from the ü§ó Datasets library to resample our data.</p>","metadata":{}},{"cell_type":"code","source":"from datasets import Audio\n\n# Resampling data\ndf = df.cast_column(\"audio\", Audio(sampling_rate = 16000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking new sampling rate\nprint('\\n* * * Dataset After Resampling* * *')\nprint('\\n' * 3)\nprint(\"file:\", df['train'][2]['file']) \nprint('\\n')\nprint(\"audio: \", df['train'][2]['audio']) \nprint('\\n')\nprint(\"genre:\", df['train'][2]['genre']) \nprint('\\n')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">You can now see that <code>sampling_rate: 16000</code>. Our resampling method was successful.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">As you can see in the output above, the audio data is represented as a one-dimensional array (<code>[ 0.03352892,  0.05795968,  0.09609947, ..., -0.03805662,0.00101842,  0.01898582], dtype=float32</code>). For our model to converge during training, we must ensure that all inputs are within the <b>same range</b>. We will need to normalize our data by rescaling each sample to have mean $0$ and variance $1$.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's take a look at one example in the dataset.</p>","metadata":{}},{"cell_type":"code","source":"# Computing mean and variance \nsample = df['train'][9]['audio']\n\nprint('\\nExample Data:\\n')\nprint(f\" Mean: {np.mean(sample['array']):.3}\\n\")\nprint(f\" Variance: {np.var(sample['array']):.3}\\n\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Our mean is extremely close to 0.0, but the variance is not even close to 1.0. We can apply <code>feature_extractor</code> to normalize the data.</p>","metadata":{}},{"cell_type":"code","source":"# Normalizing audio data\ninputs = feature_extractor(sample['array'], sampling_rate = sample['sampling_rate'])\n\nprint(f\"\\nInputs Keys: {list(inputs.keys())}\\n\")\n\nprint(\n    f\" Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}\"\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Success! We now have variance $1.0$. </p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">By appling <code>feature_extractor</code>, we obtain a dictionary of two arrays: <code>input_values</code> and <code>attention_mask</code>. The <code>input_values</code> array contains the preprocessed audio inputs that we will pass to our HuBERT model.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">By now, we have only applied the normalization to one single sample. Let's build a preprocessing function, so we can perform the steps above to the entire dataset at once.</p>","metadata":{}},{"cell_type":"code","source":"# Defining audio maximum duration\nmax_duration = 30.0 # 30 seconds\n\ndef preprocess_function(examples):\n    # Extracting and saving arrays \n    audio_arrays = [x['array'] for x in examples['audio']]\n    \n    # Preprocessing audio inputs\n    inputs = feature_extractor(audio_arrays,\n                              sampling_rate = feature_extractor.sampling_rate,\n                              max_length = int(feature_extractor.sampling_rate * max_duration),\n                              truncation = True,\n                              return_attention_mask = True)\n    \n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's use the <code>map( )</code> method to apply the function above to the dataset. We will remove some columns that will not be used during training and validation, as well as define a batch size.</p>","metadata":{}},{"cell_type":"code","source":"# Preprocessing data\ndf = df.map(preprocess_function,\n                   remove_columns = ['audio', 'file'],\n                   batched = True,\n                   batch_size = 100,\n                   num_proc = 1)\n\ndf # Visualizing results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <div style=\"\n    background-color:#3b3745;\n    text-align:left; \n    padding: 13px; \n    border-radius: 22px;\n    font-size: 14px;\n    font-weight: 300;\n    color: white;\n    opacity: 1\"> <b>üìù The `batch_size` parameter is important to define the peak RAM usage. If the code above exhaust your device's RAM, you can iteratively reduce it by a factor of 2 until it works fine for you.</b></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">After preprocessing the dataset, we are left with three features:</p>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"margin-left: 25px\">\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212;\"><b>‚Ä¢ genre:</b> This is the target variable. This contains all the genre labels we will fine-tune our model to classify audio files.</p>\n    \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212;\"><b>‚Ä¢ input_values:</b> This contains the encoded audio files, so the HuBERT model can accurately <i>understand</i> the data it receives.</p>\n    \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212;\"><b>‚Ä¢ attention_mask:</b> This feature consists of binary 0 and 1 values that indicade where we have padded the audio input.</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">To enable the <code>Trainer</code> class to process labels, we must rename this column to <code>label</code>.</p>","metadata":{}},{"cell_type":"code","source":"# Renaming genre column\ndf = df.rename_column('genre', 'label')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The last step in our preprocessing journey is mapping our genre labels from integer IDs to human-readable descriptions(e.g., $7$ -> $pop$) and vice-versa. This can be easily done by using the <code>int2str( )</code> method</p>","metadata":{}},{"cell_type":"code","source":"# Id to label\nid2label = {str(i): id2label_function(i)\n           for i in range(len(df['train'].features['label'].names))}\n\n# Label to id\nlabel2id = {v: k for k, v in id2label.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's test it.</p>","metadata":{}},{"cell_type":"code","source":"integer = 7 # Defining a random int\nlabel = id2label[str(integer)] # Obtaining label \n\nprint(f'\\nId: {integer}')\nprint(f'\\nLabel: {label}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">It is working as intended.</p>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'fine-tuning'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 56px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\n                border-bottom: 1.25px solid #121212\"><b>Fine-tuning HuBERT</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img style=\"float:left; margin-top: 0;\n    margin:0; \n    padding:20px;\" src = \"https://i.imgur.com/pWBYP55.png\" width = 230, height= 230>\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The data is preprocessed and ready to be used for fine-tuning the HuBERT model for the task of music genre classification.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">For this task, we are going to use ü§ó Transformers' <code>Trainer</code> and <code>TrainingArguments</code>.</p>\n\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Let's go ahead and use <code>AutoModelForAudioClassification</code> to load our model with the appropriate classification head for this task and visualize its architecture.</p>","metadata":{}},{"cell_type":"code","source":"num_labels = len(id2label) # Obtaining the total number of labels\n\n# Loading model\nhubert_model = AutoModelForAudioClassification.from_pretrained(model,\n                                                         num_labels = num_labels,\n                                                         label2id=label2id,\n                                                         id2label=id2label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing model's architecture\nprint('\\nHuBERT Architecture')\nprint(hubert_model) ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">I will now set the training arguments and define a function for accuracy computing. We can then start the fine-tuning process.</p>","metadata":{}},{"cell_type":"code","source":"# Setting training arguments\ntraining_args = TrainingArguments(\n    output_dir = 'hubert_gtzan',\n    evaluation_strategy = 'epoch',\n    save_strategy = 'epoch',\n    load_best_model_at_end = True,\n    metric_for_best_model = 'accuracy',\n    learning_rate = 5e-5,\n    seed = seed,\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    gradient_accumulation_steps = 1,\n    num_train_epochs = 15,\n    warmup_ratio = 0.1,\n    fp16 = True,\n    save_total_limit = 2,\n    report_to = 'none'\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <div style=\"\n    background-color:#3b3745;\n    text-align:left; \n    padding: 13px; \n    border-radius: 22px;\n    font-size: 14px;\n    font-weight: 300;\n    color: white;\n    opacity: 1\"> <b>üìù If you encounter a CUDA <i>out-of-memory</i> error during training, you can reduce `batch_size` and increase `gradient_acumulation_steps` to solve the problem.</b></div>","metadata":{}},{"cell_type":"code","source":"# Loading `accuracy` metric from the evaluate library\nmetric = evaluate.load('accuracy')\n# Creating function to compute accuracy\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis = 1)\n    return metric.compute(predictions = predictions, references = eval_pred.label_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting trainer\ntrainer = Trainer(\n    model=hubert_model, \n    args = training_args,\n    train_dataset = df['train'],\n    eval_dataset = df['test'],\n    tokenizer = feature_extractor,\n    compute_metrics = compute_metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Our best evaluation accuracy is <b><i>78.5%</i></b>. We can now save our model and the feature extractor for further use, model deploy, and even upload on Hugging Face Models so others can fine-tune it even further on their own tasks and data.</p>","metadata":{}},{"cell_type":"code","source":"# Saving model & feature extractor\ndirectory = 'fine_tuned_hubert'\ntrainer.save_model(directory)\nfeature_extractor.save_pretrained(directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">The model trained in this notebook has been uploaded to Hugging Face and you can use it in your own applications. Just click on <b><i><a href = \"https://huggingface.co/luisotorres/hubert_gtzan\">luisotorres/hubert_gtzan</a></i></b> to check it.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">You can also load it on an audio-classification pipeline to test it using your own custom audios.</p>","metadata":{}},{"cell_type":"markdown","source":"> <div style=\"\n    background-color:#3b3745;\n    text-align:left; \n    padding: 13px; \n    border-radius: 22px;\n    font-size: 14px;\n    font-weight: 300;\n    color: white;\n    opacity: 1\"> <i><b>from transformers import pipeline<br><br>\npipe = pipeline(\n    \"audio-classification\", model=\"luisotorres/hubert_gtzan\"\n)</b></i></div>","metadata":{}},{"cell_type":"markdown","source":"<div id = 'conclusion'\n     style=\"font-family: 'Segoe UI', sans-serif; text-align: left;\">\n    <div style=\"font-size: 56px; letter-spacing: 4.25px;color: #121212;\n                text-shadow: 0 2px 2px rgba(0,0,0,0.2);\n                border-bottom: 1.25px solid #121212\"><b>Conclusion</b></div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img style=\"float:left; margin-top: 0;\n    margin:0; \n    padding:20px;\" src = \"https://i.imgur.com/cq7Vl9d.png\" width = 230, height= 230>\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">In this notebook, we have explored the details behind <b>audio processing</b> and how <b>Transformer</b> architectures can be efficiently used in several audio tasks.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">More specifically, we have employed the convenience of the ü§ó Transformers library to fine-tune a distilled version of the <b>HuBERT</b> model on the <b>GTZAN</b> dataset for the task of <b>audio classification</b>.</p>\n\n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">I truly hope this notebook serves as a good introduction for those interested in leveraging deep learning frameworks for working with audio data, as well as a good source for those who are veterans in search of refining their knowledge on the subject.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Feel free to leave your comments, suggestions, and ideas. Your feedback is highly appreciated!</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">If you like the content presented in this notebook, your upvote is also extremely appreciated.</p>\n          \n<p style=\"font-family: 'Segoe UI', san-serif; text-align: left;\n          font-size: 24px; letter-spacing: .85px;color: #121212; margin-top: 40px\">Thank you!</p>","metadata":{}},{"cell_type":"markdown","source":"<hr style=\"border: 0; \n           height: 1px; \n           border-top: 0.85px; \n           solid #b2b2b2\">\n           \n<div style=\"text-align: left; \n            color: #8d8d8d; \n            padding-left: 15px; \n            font-size: 14.25px;\">\n    Luis Fernando Torres, 2023 <br><br>\n    Let's connect!üîó<br>\n    <a href=\"https://www.linkedin.com/in/luuisotorres/\">LinkedIn</a> ‚Ä¢ <a href=\"https://medium.com/@luuisotorres\">Medium</a> ‚Ä¢ <a href = \"https://huggingface.co/luisotorres\">Hugging Face</a><br><br>\n</div>\n<div style=\"text-align: center; \n            margin-top: 50px;\n            color: #8d8d8d; \n            padding-left: 15px; \n            font-size: 14.25px;\"><b>Like my content? Feel free to <a href=\"https://www.buymeacoffee.com/luuisotorres\">Buy Me a Coffee ‚òï</a></b>\n</div>\n<div style=\"text-align: center; \n            margin-top: 80px;\n            color: #8d8d8d; \n            padding-left: 15px; \n            font-size: 14.25px;\"><b>  <a href = \"https://luuisotorres.github.io/\">https://luuisotorres.github.io/</a> </b>\n</div>","metadata":{}}]}